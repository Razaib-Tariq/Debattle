{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd64bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Razai/one_to_one_trim5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fer:30.00 fps, 154 frames, 5.13 seconds\n",
      "INFO:fer:Making directories at output\n",
      "INFO:fer:Deleted pre-existing output\\one_to_one_trim5_output.mp4\n",
      "  0%|                                                                                      | 0/154 [00:00<?, ?frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1482 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000202C53439D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1482 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000202C53439D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 154/154 [00:54<00:00,  2.80frames/s]\n",
      "INFO:fer:Completed analysis: saved to output\\one_to_one_trim5_output.mp4\n",
      "INFO:fer:Starting to Zip\n",
      "INFO:fer:Compressing: 32%\n",
      "INFO:fer:Compressing: 64%\n",
      "INFO:fer:Compressing: 97%\n",
      "INFO:fer:Zip has finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-030dfb5e00b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"Neutral\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnew_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m }\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.mp4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_output\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".mp4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'location' is not defined"
     ]
    }
   ],
   "source": [
    "from fer import FER\n",
    "import matplotlib.pyplot as plt \n",
    "from fer import Video\n",
    "from fer import FER\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "location_videofile = \"C:/Users/Razai/one_to_one_trim5.mp4\"\n",
    "print(location_videofile)\n",
    "face_detector = FER(mtcnn=True)\n",
    "# Input the video for processing\n",
    "input_video = Video(location_videofile)\n",
    "    \n",
    "# The Analyze() function will run analysis on every frame of the input video. \n",
    "# It will create a rectangular box around every image and show the emotion values next to that.\n",
    "# Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\n",
    "processing_data = input_video.analyze(face_detector, display=False)\n",
    "\n",
    "# We will now convert the analysed information into a dataframe.\n",
    "# This will help us import the data as a .CSV file to perform analysis over it later\n",
    "vid_df = input_video.to_pandas(processing_data)\n",
    "vid_df = input_video.get_first_face(vid_df)\n",
    "vid_df = input_video.get_emotions(vid_df)\n",
    "# We will now work on the dataframe to extract which emotion was prominent in the video\n",
    "angry = sum(vid_df.angry)\n",
    "disgust = sum(vid_df.disgust)\n",
    "fear = sum(vid_df.fear)\n",
    "happy = sum(vid_df.happy)\n",
    "sad = sum(vid_df.sad)\n",
    "surprise = sum(vid_df.surprise)\n",
    "neutral = sum(vid_df.neutral)\n",
    "import numpy as np\n",
    "arr = np.array([angry,disgust,fear,happy,sad,surprise,neutral])\n",
    "result = np.linalg.norm(arr)\n",
    "new_output=arr/result\n",
    "res={\"Angry\":new_output[0]*100\n",
    ",\"Disgust\":new_output[1]*100\n",
    ",\"Fear\":new_output[2]*100\n",
    ",\"Happy\":new_output[3]*100\n",
    ",\"Sad\":new_output[4]*100\n",
    ",\"Surprise\":new_output[5]*100\n",
    ",\"Neutral\":new_output[6]*100\n",
    "}\n",
    "output=location.replace('.mp4', '')\n",
    "output=output+\"_output\"+\".mp4\"\n",
    "os.remove(location)\n",
    "os.remove('data.csv')\n",
    "os.remove('output/'+output)\n",
    "os.remove('output/'+\"images.zip\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb9c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
